import glob
import json
from collections import Counter
from datetime import datetime
import os
from pathlib import Path
import seaborn as sns
import joblib
import numpy as np
import pandas as pd
from typing import List, Dict, Type
from catboost import Pool
from notebooks.models.catboost_model import CatBoost
from notebooks.models.lightgbm_model import LightGBM
from matplotlib import pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, classification_report, roc_curve, auc, confusion_matrix
from sklearn.model_selection import cross_val_predict

from notebooks.feature_selection import MetricNames
from notebooks.models.helpers import find_optimal_threshold
from notebooks.models.model import Model


class FinalEnsemble:
    def __init__(self, model_classes: List[Type[Model]], use_stacking: bool = True):
        self.test = None
        self.train = None
        self.models = [model_class() for model_class in model_classes]
        self.use_stacking = use_stacking
        self.stacking_model = LogisticRegression(
            class_weight={0: 1, 1: 33},
            C=0.1,
            solver='lbfgs',
            max_iter=1000
        ) if use_stacking else None
        self.ensemble_dir = self._prepare_ensemble_dir()

    def _prepare_ensemble_dir(self) -> Path:
        """–°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è"""
        piece_path = Path('hard').joinpath('sub8')
        base_dir = Path(__file__).parents[2] / 'data' / 'models' / 'ensemble'
        ensemble_dir = base_dir / piece_path / datetime.now().strftime("%Y%m%d_%H%M%S")
        ensemble_dir.mkdir(parents=True, exist_ok=True)
        return ensemble_dir

    def load_model_params(self, model: Model, feature_metric: str, part: int, optimisation_metric: str) -> Dict:
        """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        piece_path = Path('hard').joinpath('sub8')
        params_dir = (
                Path(__file__).parents[2] / 'data' / 'models' / 'hyper' /
                model.name / piece_path / feature_metric / str(part) / optimisation_metric
        )
        print(params_dir)
        files = glob.glob(f"{params_dir}/best_params_*.json")
        latest_file = max(files, key=os.path.getctime)
        params = json.load(open(latest_file, 'r'))
        return params

    def prepare_data(self, feature_metric: str, part: int, optimisation_metric: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        piece_path = Path('hard').joinpath('sub8')
        base_path = Path(__file__).parents[2] / 'data' / 'datasets' / piece_path

        self.train = pd.read_parquet(base_path.joinpath('cross.parquet'))
        self.test = pd.read_parquet(base_path.joinpath('test.parquet'))

        for model in self.models:
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            model.params = {
                **self.load_model_params(model, feature_metric, part, optimisation_metric),
            }

            model.train = self.train
            model.test = self.test

    def _generate_meta_features(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        meta_features = np.zeros((self.train.shape[0], len(self.models)))

        for i, model in enumerate(self.models):
            try:
                meta_features[:, i] = model.get_meta()
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ {model.name}: {str(e)}")
                raise

        return meta_features

    def train_models(self):
        """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        for model in self.models:
            print(f"\nüåÄ –û–±—É—á–µ–Ω–∏–µ {model.name}...")
            # –í—ã–∑–æ–≤ —Ä–æ–¥–Ω–æ–≥–æ fit() –º–æ–¥–µ–ª–∏
            print(f'–ì–∏–ø–µ—Ä–∞: {model.hyper_params}')
            model.fit()

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
            model_path = self.ensemble_dir / f"{model.name}.cbm"
            model.save_model(model_path)
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

        if self.use_stacking:
            print("\nüåÄ –û–±—É—á–µ–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥-–º–æ–¥–µ–ª–∏...")

            meta_train = self._generate_meta_features()

            self.stacking_model.fit(meta_train, self.train['target'])

            stacking_path = self.ensemble_dir / "stacking_model.joblib"
            joblib.dump(self.stacking_model, stacking_path)
            print(f"üíæ –°—Ç–µ–∫–∏–Ω–≥-–º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {stacking_path}")

    # def _detect_categorical_features(self) -> List[str]:
    #     """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    #     return [
    #         col for col in self.train.columns
    #         if self.train[col].dtype == 'object' or self.train[col].nunique() < 10
    #     ]

    def evaluate_ensemble(self):
        """–û—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å—Ç–µ–∫–∏–Ω–≥–∞"""
        metrics = {}

        print('–û—Ü–µ–Ω–∫–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π')
        for model in self.models:
            model_metrics, pred = self._get_model_metrics(model)
            metrics[model.name] = model_metrics
            self._print_model_metrics(model.name, model_metrics, pred)

        print('–û—Ü–µ–Ω–∫–∞ —Å—Ç–µ–∫–∏–Ω–≥–∞ –µ—Å–ª–∏ –µ—Å—Ç—å')
        if self.use_stacking:
            stacking_metrics, pred = self._get_stacking_metrics()
            metrics['stacking'] = stacking_metrics
            print(stacking_metrics)
            self._print_model_metrics('Stacking', stacking_metrics, pred)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫
        metrics_path = self.ensemble_dir / "metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=4)
        print(f"\nüíæ –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_path}")

    def _get_model_metrics(self, model: Model) -> (Dict, Dict):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        X_test = self.test[model.features].copy()
        X_test = model.convert_categories(X_test)
        # if model.name == 'lightgbm':
        #     for col in X_test.select_dtypes(include=['object']):
        #         X_test[col] = X_test[col].astype('category')
        y_proba = model.model.predict_proba(X_test)[:, 1]

        optimal_threshold, _ = find_optimal_threshold(self.test['target'], y_proba)

        # –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º optimal_pred –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Ä–æ–≥–∞
        optimal_pred = (y_proba >= optimal_threshold).astype(int)

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º (0.5)
        report_default = classification_report(
            self.test['target'],
            (y_proba >= 0.5).astype(int),
            target_names=['Class 0', 'Class 1'],
            output_dict=True
        )

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
        report_optimal = classification_report(
            self.test['target'],
            optimal_pred,  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ –º–∞—Å—Å–∏–≤ 0 –∏ 1
            target_names=['Class 0', 'Class 1'],
            output_dict=True
        )

        return {
            'auc': roc_auc_score(self.test['target'], y_proba),
            'optimal_threshold': optimal_threshold,
            'report_default': report_default,
            'report_optimal': report_optimal
        }, {
            'default': y_proba,
            'optimal': optimal_pred,
        }

    def _get_stacking_metrics(self) -> (Dict, Dict):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
        """ ===== –ü–†–û–í–ï–†–û–ß–ù–´–ô –ö–û–î ====="""
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —Å—Ç–µ–∫–∏–Ω–≥–æ–º"""
        print("\n=== –≠–ö–°–ü–†–ï–°–°-–ü–†–û–í–ï–†–ö–ê –î–ê–ù–ù–´–• ===")

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        print("–ö–ª–∞—Å—Å—ã –≤ target:",
              f"0={sum(self.test['target'] == 0)}, 1={sum(self.test['target'] == 1)}")

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏—á–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for model in self.models:
            X_test = self.test[model.features]
            print(f"\n–ú–æ–¥–µ–ª—å {model.name}:")
            print("–§–∏—á–∏:", X_test.shape[1], "| –ü—Ä–∏–º–µ—Ä—ã:", X_test.shape[0])
            print("–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:", X_test.dtypes.value_counts().to_dict())

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ NaN/Inf
            print("–ü—Ä–æ–±–ª–µ–º—ã –≤ –¥–∞–Ω–Ω—ã—Ö:",
                  f"NaN={X_test.isna().sum().sum()}",
                  f"Inf={(X_test.values == np.inf).sum()}")

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–µ–∫–∏–Ω–≥-–º–æ–¥–µ–ª–∏
        if self.use_stacking:
            print("\n–°—Ç–µ–∫–∏–Ω–≥-–º–æ–¥–µ–ª—å:",
                  f"–ì–æ—Ç–æ–≤–∞={hasattr(self.stacking_model, 'predict_proba')}")

        """ ===== –ö–û–ù–ï–¶ –ü–†–û–í–ï–†–û–ß–ù–û–ì–û –ö–û–î–ê ====="""
        try:
            print('–°–±–æ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π')
            meta_test = []
            for model in self.models:
                X_test = model.convert_categories(self.test[model.features])
                proba = model.model.predict_proba(X_test)[:, 1]
                meta_test.append(proba)

                # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
                print(f"\nüîç {model.name}:")
                print(f"Predictions min={proba.min():.3f} max={proba.max():.3f}")
                print(f"Mean={proba.mean():.3f} | Std={proba.std():.3f}")
                print(f"–ö–æ–ª-–≤–æ 0={(proba < 0.5).sum()} | –ö–æ–ª-–≤–æ 1={(proba >= 0.5).sum()}")

            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            meta_test = np.column_stack(meta_test)
            print("\nüî• –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏:")
            print(f"–†–∞–∑–º–µ—Ä: {meta_test.shape} (–ø—Ä–∏–º–µ—Ä—ã √ó –º–æ–¥–µ–ª–∏)")
            print("–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 3 —Å—Ç—Ä–æ–∫:")
            print(pd.DataFrame(meta_test).head(3).to_string())

            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ "–º—É—Å–æ—Ä"
            print("\nüßπ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–±–ª–µ–º:")
            print(f"NaN: {np.isnan(meta_test).sum()}")
            print(f"Inf: {np.isinf(meta_test).sum()}")
            print(f"–í—Å–µ –Ω—É–ª–∏: {(meta_test == 0).all(axis=0).sum()} –∫–æ–ª–æ–Ω–æ–∫")

            print('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥-–º–æ–¥–µ–ª–∏')
            y_proba = self.stacking_model.predict_proba(meta_test)[:, 1]
            if np.all(y_proba == 0):
                raise ValueError("–°—Ç–µ–∫–∏–Ω–≥-–º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –≤—Å–µ –Ω—É–ª–∏")
            print("\n=== –≠–ö–°–ü–†–ï–°–°-–ü–†–û–í–ï–†–ö–ê –î–ê–ù–ù–´–• ===")

            print('–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫')
            y_pred_standard = (y_proba >= 0.5).astype(int)
            optimal_threshold, _ = find_optimal_threshold(self.test['target'], y_proba)
            y_pred_optimal = (y_proba >= optimal_threshold).astype(int)
            print(type(optimal_threshold))
            return {
                'auc': float(roc_auc_score(self.test['target'], y_proba)),
                'report': classification_report(
                    self.test['target'],
                    y_pred_standard,
                    target_names=['Class 0', 'Class 1'],
                    output_dict=True
                ),
                'optimal_threshold': float(optimal_threshold),
                'optimal_report': classification_report(
                    self.test['target'],
                    y_pred_optimal,
                    target_names=['Class 0', 'Class 1'],
                    output_dict=True
                ),
                'proba_stats': {
                    'min': float(y_proba.min()),
                    'max': float(y_proba.max()),
                    'mean': float(y_proba.mean()),
                    'std': float(y_proba.std())
                }
            }, {
                'default': y_proba,
                'optimal': y_pred_optimal,
            }

        except Exception as e:
            print(f"üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å—Ç–µ–∫–∏–Ω–≥–∞: {str(e)}")
            return {
                'auc': 0,
                'error': str(e),
                'proba_stats': None
            }

    def _print_model_metrics(self, name: str, metrics: Dict, pred: Dict):
        """–í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–∏"""
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {name}:")
        print(f"üìä ROC-AUC: {metrics['auc']:.4f}")
        print(f"üìå –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {metrics['optimal_threshold']:.4f}")

        print("\nüìù Classification Report (–ø–æ—Ä–æ–≥ 0.5):")
        print(classification_report(
            self.test['target'],
            (pred['default'] >= 0.5).astype(int),
            target_names=['Class 0', 'Class 1']
        ))

        print("\nüìù Classification Report (–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥):")
        print(classification_report(
            self.test['target'],
            pred['optimal'],
            target_names=['Class 0', 'Class 1']
        ))


def final_ensemble_learn(use_stacking=True):
    print('–í–∞—Ä–∏–∞–Ω—Ç 1')
    ensemble = FinalEnsemble(
        model_classes=[CatBoost, LightGBM],
        use_stacking=use_stacking
    )
    ensemble.prepare_data(
        feature_metric=MetricNames.f1_1,
        part=0,
        optimisation_metric=MetricNames.f1_1
    )
    ensemble.train_models()
    ensemble.evaluate_ensemble()


if __name__ == "__main__":
    final_ensemble_learn()
